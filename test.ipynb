{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fasttext\n",
    "from typing import List, Callable, Tuple\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import contractions\n",
    "import concurrent.futures\n",
    "import re\n",
    "import keras\n",
    "from keras import models,layers,optimizers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.layers import TextVectorization, Input, Conv1D"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T15:48:50.841467700Z",
     "start_time": "2023-09-26T15:48:42.825494300Z"
    }
   },
   "id": "22b0d143efcfca2"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "BASE_FOLDER = 'resources'\n",
    "PREPROCESSED_DATA = 'preprocessed'\n",
    "DATASET = f'{BASE_FOLDER}/dataset.csv'\n",
    "ONE_LANGUAGE_DATASET = f'{BASE_FOLDER}/{PREPROCESSED_DATA}/only_english_dataset.csv'\n",
    "PREPROCESSED_DATASET = f'{BASE_FOLDER}/{PREPROCESSED_DATA}/preprocessed_dataset.csv'\n",
    "COLUMN_TO_CONSIDER = 'sentiment'\n",
    "MAX_STR_LENGTH = 300"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T15:48:27.158856800Z",
     "start_time": "2023-09-26T15:48:27.143853600Z"
    }
   },
   "id": "e1d5809d56582b45"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gardo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def apply_concurrent(function: Callable[[str], str], workers:int = os.cpu_count()):\n",
    "  def _preprocess_series(series: pd.Series):\n",
    "    with concurrent.futures.ThreadPoolExecutor(workers) as executor:\n",
    "        return pd.Series(list(executor.map(function, series.values)))\n",
    "  return _preprocess_series\n",
    "\n",
    "def compute_languages():\n",
    "  model = fasttext.load_model('resources/lid.176.bin')\n",
    "  def _compute_languages(texts: pd.Series):\n",
    "    return texts.apply(lambda x:model.predict(x)[0][0])\n",
    "  return _compute_languages\n",
    "\n",
    "def remove_not_modal_languages(extracted_languages: pd.Series):\n",
    "  def _remove_languages(dataframe: pd.DataFrame):\n",
    "    modal_language = extracted_languages.value_counts().index[0]\n",
    "    return dataframe[extracted_languages == modal_language].reset_index(drop=True)\n",
    "  return _remove_languages\n",
    "\n",
    "def remove_lxml():\n",
    "    def _remove_lxml(text):\n",
    "        return BeautifulSoup(text, 'lxml').get_text().strip()\n",
    "    return _remove_lxml\n",
    "\n",
    "def expand():\n",
    "    def _expand(text):\n",
    "        return contractions.fix(text)\n",
    "    return _expand\n",
    "    \n",
    "def map_text(mapping_function):\n",
    "    def _map_text(text):\n",
    "        return mapping_function(text)\n",
    "    return _map_text\n",
    "\n",
    "def remove_links():\n",
    "    def _remove_links(text):\n",
    "        return re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', \"\", text)\n",
    "    return _remove_links\n",
    "  \n",
    "def remove_mails():\n",
    "    def _remove_mails(text):\n",
    "        return re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)', \"\", text)\n",
    "    return _remove_mails      \n",
    "\n",
    "\n",
    "def generic_regex(pattern, repl):\n",
    "    def _generic_regex(text):\n",
    "        return re.sub(pattern, repl, text)\n",
    "    return _generic_regex\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def stopwords_list(language='english'):\n",
    "    return stopwords.words(language)\n",
    "\n",
    "\n",
    "def preprocess():\n",
    "    stopword_regex = r'\\b(' + r'|'.join(stopwords_list()) + r')\\b\\s*'\n",
    "    remove_lxml_fn = remove_lxml()\n",
    "    def _preprocess(text):\n",
    "        functions = [\n",
    "            map_text(lambda x: x.lower()),\n",
    "            remove_links(), # remove links\n",
    "            remove_mails(), # remove mails\n",
    "            remove_lxml_fn,\n",
    "            #expand(),\n",
    "            generic_regex('[^a-z]', ' '), # remove all non chars\n",
    "            generic_regex(r'\\s+[a-z]\\s+', ' '), # remove all isolated characters\n",
    "            generic_regex(stopword_regex, ''), # remove stopwords\n",
    "            generic_regex(r'\\s+', ' ') # remove extra white spaces\n",
    "        ]\n",
    "        for function in functions:\n",
    "            text = function(text)   \n",
    "        return text\n",
    "    return _preprocess"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T15:58:18.162156900Z",
     "start_time": "2023-09-26T15:58:17.995496700Z"
    }
   },
   "id": "d1867154ab85fa4f"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('resources/train.csv')\n",
    "dataset.columns = ['sentiment', 'title', 'text']\n",
    "dataset = dataset.head(500_000)\n",
    "dataset['text'] = dataset['title'] + \" \" + dataset['text']\n",
    "del dataset['title']\n",
    "dataset = dataset.dropna()\n",
    "languages = compute_languages()(dataset['text'])\n",
    "dataset = remove_not_modal_languages(languages)(dataset)\n",
    "dataset = dataset.head(500_000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T16:51:50.510372600Z",
     "start_time": "2023-09-26T16:51:19.233966600Z"
    }
   },
   "id": "5d560a3bad000832"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def normalise(texts):\n",
    "    return [re.compile(r\"[^a-z0-1\\s]\").sub(r\" \",re.compile(r\"['\\W']\").sub(r\" \",text.lower())) for text in texts]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T16:53:21.538416600Z",
     "start_time": "2023-09-26T16:53:21.525413600Z"
    }
   },
   "id": "1e2c138ebc007f2e"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "dataset['text'] = normalise(dataset['text'].values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T16:53:40.765711100Z",
     "start_time": "2023-09-26T16:53:26.116311200Z"
    }
   },
   "id": "e06e79f710ecd967"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-26T12:52:56.586547Z",
     "start_time": "2023-09-26T12:52:40.706498800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One language dataset loaded\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(ONE_LANGUAGE_DATASET):\n",
    "    print('Creating one language dataset')\n",
    "    dataset = pd.read_csv(DATASET)\n",
    "    dataset.columns = ['stars', 'title', 'text']\n",
    "    \n",
    "    dataset['text'] = dataset['title'] + \" \" + dataset['text']\n",
    "    del dataset['title']\n",
    "    \n",
    "    dataset = dataset.dropna()\n",
    "    \n",
    "    languages = compute_languages()(dataset['text'])\n",
    "    dataset = remove_not_modal_languages(languages)(dataset)\n",
    "    \n",
    "    dataset['sentiment'] = dataset['stars'].replace({1: 1, 2: 1, 3: 2, 4: 3, 5: 3})\n",
    "    \n",
    "    dataset.to_csv(ONE_LANGUAGE_DATASET, index=False)\n",
    "    print('One language dataset saved')\n",
    "else:\n",
    "    dataset = pd.read_csv(ONE_LANGUAGE_DATASET)\n",
    "    print('One language dataset loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "         stars                                               text  sentiment\n0            5  Inspiring I hope a lot of people hear this cd....          3\n1            5  The best soundtrack ever to anything. I'm read...          3\n2            4  Chrono Cross OST The music of Yasunori Misuda ...          3\n3            5  Too good to be true Probably the greatest soun...          3\n4            5  There's a reason for the price There's a reaso...          3\n...        ...                                                ...        ...\n2991668      1  Don't do it!! The high chair looks great when ...          1\n2991669      2  Looks nice, low functionality I have used this...          1\n2991670      2  compact, but hard to clean We have a small hou...          1\n2991671      3  Hard to clean! I agree with everyone else who ...          2\n2991672      1  what is it saying? not sure what this book is ...          1\n\n[2991673 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stars</th>\n      <th>text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>Inspiring I hope a lot of people hear this cd....</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>The best soundtrack ever to anything. I'm read...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>Chrono Cross OST The music of Yasunori Misuda ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>Too good to be true Probably the greatest soun...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>There's a reason for the price There's a reaso...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2991668</th>\n      <td>1</td>\n      <td>Don't do it!! The high chair looks great when ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2991669</th>\n      <td>2</td>\n      <td>Looks nice, low functionality I have used this...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2991670</th>\n      <td>2</td>\n      <td>compact, but hard to clean We have a small hou...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2991671</th>\n      <td>3</td>\n      <td>Hard to clean! I agree with everyone else who ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2991672</th>\n      <td>1</td>\n      <td>what is it saying? not sure what this book is ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2991673 rows Ã— 3 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T12:52:56.649050600Z",
     "start_time": "2023-09-26T12:52:56.586547Z"
    }
   },
   "id": "e819e6aea7269ba3"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset loaded\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(PREPROCESSED_DATASET):\n",
    "    print('Preprocessing dataset')\n",
    "    preprocessed_values = [preprocess()(text) for text in tqdm(dataset['text'], \n",
    "                                                               position=0, \n",
    "                                                               leave=True)]\n",
    "    dataset['text'] = preprocessed_values\n",
    "    dataset.to_csv(PREPROCESSED_DATASET, index=False)\n",
    "    print('Preprocessed dataset saved')\n",
    "else:\n",
    "    dataset = pd.read_csv(PREPROCESSED_DATASET)\n",
    "    print('Preprocessed dataset loaded')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T13:37:46.387627300Z",
     "start_time": "2023-09-26T13:37:30.671450300Z"
    }
   },
   "id": "691c58680ca0b9f9"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "0    inspiring hope lot of people hear this cd  we ...\n1    the best soundtrack ever to anything m reading...\n2    chrono cross ost the music of yasunori misuda ...\nName: text, dtype: object"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'][:3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T13:37:46.403253200Z",
     "start_time": "2023-09-26T13:37:46.387627300Z"
    }
   },
   "id": "dfb75d60af0230ca"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# # Create and print a Reviews length distribution graph.\n",
    "# review_length_distribution_plt = pd.DataFrame(dataset[\"text\"].str.len())\n",
    "# review_length_distribution_plt = review_length_distribution_plt[review_length_distribution_plt['text'] < 5000]\n",
    "# review_length_distribution_plt.groupby([\"text\"])\n",
    "# review_length_distribution_plt = review_length_distribution_plt.plot(kind='hist', \n",
    "#                                                                      legend=None, \n",
    "#                                                                      bins=50, \n",
    "#                                                                      figsize=(12, 6))\n",
    "# review_length_distribution_plt.set_xlabel(\"Review Length\")\n",
    "# review_length_distribution_plt.set_ylabel(\"Count\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T13:37:46.449153100Z",
     "start_time": "2023-09-26T13:37:46.403253200Z"
    }
   },
   "id": "2c0077a04505d8a4"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "n_classes = len(set(dataset[COLUMN_TO_CONSIDER]))\n",
    "X = dataset['text'].astype(str).values\n",
    "Y = (dataset[COLUMN_TO_CONSIDER] - 1).values\n",
    "if n_classes == 2:\n",
    "    n_classes = 1\n",
    "    activation_function = 'sigmoid'\n",
    "    loss = 'binary_crossentropy'\n",
    "    y = Y\n",
    "else:\n",
    "    activation_function = 'softmax'\n",
    "    loss = 'categorical_crossentropy'\n",
    "    y = keras.utils.to_categorical(dataset[COLUMN_TO_CONSIDER] - 1, num_classes=n_classes)\n",
    "\n",
    "# train_x, test_x, train_y, test_y = train_test_split(X, y, \n",
    "#                                                     test_size=0.2, \n",
    "#                                                     stratify=y, \n",
    "#                                                     random_state=1234)\n",
    "\n",
    "train_x, validation_x, train_y, validation_y = train_test_split(train_x, train_y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify=train_y, \n",
    "                                                    random_state=1234)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T16:54:21.245612500Z",
     "start_time": "2023-09-26T16:54:20.647098200Z"
    }
   },
   "id": "251467c1c881e941"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# train_size = len(train_y)\n",
    "# validation_size = len(validation_y)\n",
    "# test_size = len(test_y)\n",
    "# sizes = [train_size, validation_size, test_size]\n",
    "# print(f'Train size: {train_size}, validation size: {validation_size}, test size: {test_size}')\n",
    "# plt.figure(figsize=(8, 4))\n",
    "# plt.barh([''], [100], color='white')  # Create a white bar for the total percentage\n",
    "# plt.barh([''], [train_size], label='Train', color='blue')\n",
    "# plt.barh([''], [validation_size], left=train_size, label='Validation', color='green')\n",
    "# plt.barh([''], [test_size], left=train_size+validation_size, label='Test', color='red')\n",
    "# \n",
    "# plt.title('Dataset divisions')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.xticks([])\n",
    "# # Display the chart\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T17:47:48.816087500Z",
     "start_time": "2023-09-26T17:47:48.773077500Z"
    }
   },
   "id": "1e0446025d24d88"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "mlen = max(len(train_ex) for train_ex in train_x)\n",
    "vectorize_layer = TextVectorization(max_tokens=10_000, output_sequence_length=mlen)\n",
    "vectorize_layer.adapt(train_x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T16:55:12.288989200Z",
     "start_time": "2023-09-26T16:54:55.580172300Z"
    }
   },
   "id": "46eb1e573399f7b2"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# tokenizer=Tokenizer(num_words=1_000)\n",
    "# tokenizer.fit_on_texts(train_x)\n",
    "# train_texts = tokenizer.texts_to_sequences(train_x)\n",
    "# val_texts = tokenizer.texts_to_sequences(validation_x)\n",
    "# test_texts = tokenizer.texts_to_sequences(test_x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T13:31:36.625497900Z",
     "start_time": "2023-09-26T13:31:36.604134600Z"
    }
   },
   "id": "1b9a93cb60713cb1"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# mlen = max(len(train_ex) for train_ex in train_texts)\n",
    "# train_texts = pad_sequences(train_texts, maxlen=mlen)\n",
    "# val_texts = pad_sequences(val_texts, maxlen=mlen)\n",
    "# test_texts = pad_sequences(test_texts, maxlen=mlen)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T13:31:36.641124500Z",
     "start_time": "2023-09-26T13:31:36.625497900Z"
    }
   },
   "id": "5b04a61cd7f18da9"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "from keras.src.layers import Embedding, BatchNormalization, MaxPooling1D, GlobalMaxPooling1D, Activation, Dense, Dropout\n",
    "from keras import Sequential\n",
    "\n",
    "\n",
    "def build_model(n_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=(1,), dtype='string'),\n",
    "        vectorize_layer,\n",
    "        Embedding(10_000, 64),\n",
    "        \n",
    "        Conv1D(256, 3),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling1D(3),\n",
    "        \n",
    "        Conv1D(128, 5),\n",
    "        Activation('relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(5),\n",
    "        \n",
    "        Conv1D(64, 5),\n",
    "        Activation('relu'),\n",
    "        BatchNormalization(),\n",
    "        GlobalMaxPooling1D(),\n",
    "        \n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(n_classes, activation=activation_function)\n",
    "    ])\n",
    "    model.compile(optimizer='adam',loss=loss,metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_model2():\n",
    "    sequences = layers.Input(shape=(1,), dtype='string')\n",
    "    x = vectorize_layer(sequences)\n",
    "    embedded = layers.Embedding(12000, 64)(x)\n",
    "    x = layers.Conv1D(64, 3, activation='relu')(embedded)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPool1D(3)(x)\n",
    "    x = layers.Conv1D(64, 5, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPool1D(5)(x)\n",
    "    x = layers.Conv1D(64, 5, activation='relu')(x)\n",
    "    x = layers.GlobalMaxPool1D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(100, activation='relu')(x)\n",
    "    predictions = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs=sequences, outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T16:57:27.401553700Z",
     "start_time": "2023-09-26T16:57:27.364546900Z"
    }
   },
   "id": "498f70a2f8f324b3"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  29/2493 [..............................] - ETA: 10:39 - loss: 0.9432 - binary_accuracy: 0.5202"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[40], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/GPU:0\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m      4\u001B[0m     model \u001B[38;5;241m=\u001B[39m build_model2(n_classes)\n\u001B[1;32m----> 5\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_y\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m              \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m              \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m              \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mvalidation_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_y\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m              \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mEarlyStopping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmonitor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mval_loss\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpatience\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1734\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1735\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1736\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1739\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   1740\u001B[0m ):\n\u001B[0;32m   1741\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1742\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1743\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1744\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    822\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    824\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 825\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    827\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    828\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    854\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    855\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    856\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 857\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_no_variable_creation_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m    858\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    859\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    860\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[0;32m    861\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001B[0m, in \u001B[0;36mTracingCompiler.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    146\u001B[0m   (concrete_function,\n\u001B[0;32m    147\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m--> 148\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    149\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs)\u001B[0m\n\u001B[0;32m   1345\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1346\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1347\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1348\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1349\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1350\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1351\u001B[0m     args,\n\u001B[0;32m   1352\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1353\u001B[0m     executing_eagerly)\n\u001B[0;32m   1354\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001B[0m, in \u001B[0;36mAtomicFunction.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m record\u001B[38;5;241m.\u001B[39mstop_recording():\n\u001B[0;32m    195\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[1;32m--> 196\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_bound_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    198\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction_type\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflat_outputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    200\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    201\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    202\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m make_call_op_in_graph(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mlist\u001B[39m(args))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001B[0m, in \u001B[0;36mContext.call_function\u001B[1;34m(self, name, tensor_inputs, num_outputs)\u001B[0m\n\u001B[0;32m   1455\u001B[0m cancellation_context \u001B[38;5;241m=\u001B[39m cancellation\u001B[38;5;241m.\u001B[39mcontext()\n\u001B[0;32m   1456\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cancellation_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1457\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1458\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1459\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1460\u001B[0m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1461\u001B[0m \u001B[43m      \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1462\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1463\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1464\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1465\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m   1466\u001B[0m       name\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   1467\u001B[0m       num_outputs\u001B[38;5;241m=\u001B[39mnum_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1471\u001B[0m       cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_context,\n\u001B[0;32m   1472\u001B[0m   )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     52\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 53\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     56\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from keras.src.callbacks import EarlyStopping\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = build_model2(n_classes)\n",
    "    model.fit(train_x, train_y,\n",
    "              batch_size=128,\n",
    "              epochs=100,\n",
    "              validation_data=(validation_x, validation_y),\n",
    "              callbacks=[EarlyStopping(monitor='val_loss', patience=10, verbose=1)]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T16:57:37.247014900Z",
     "start_time": "2023-09-26T16:57:27.532962900Z"
    }
   },
   "id": "2e0bc9dfbcad13a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T20:14:12.355735100Z"
    }
   },
   "id": "408706a49957f02a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1ca0e19e91941055"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
